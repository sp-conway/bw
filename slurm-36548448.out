Loading r-rocker-ml-verse version 4.4.0+apptainer
Loading apptainer version latest

No apptainer cache directory found. To prevent apptainer from filling up your
home directory, you can create a new directory at
`/work/pi_<your_pi_name>/.apptainer/cache` and reload the module. 

==========
== CUDA ==
==========

CUDA Version 11.8.0

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

WARNING: The NVIDIA Driver was not detected.  GPU functionality will not be available.
   Use the NVIDIA Container Toolkit to start this container with GPU support; see
   https://docs.nvidia.com/datacenter/cloud-native/ .

here() starts at /work/pi_alc_umass_edu/spconway/scratch/bw
── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
✔ dplyr     1.1.4     ✔ readr     2.1.5
✔ forcats   1.0.0     ✔ stringr   1.5.1
✔ ggplot2   3.5.1     ✔ tibble    3.2.1
✔ lubridate 1.9.3     ✔ tidyr     1.3.1
✔ purrr     1.0.2     
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors
Loading required package: StanHeaders

rstan version 2.32.6 (Stan version 2.32.2)

For execution on a local, multicore CPU with excess RAM we recommend calling
options(mc.cores = parallel::detectCores()).
To avoid recompilation of unchanged Stan programs, we recommend calling
rstan_options(auto_write = TRUE)
For within-chain threading using `reduce_sum()` or `map_rect()` Stan functions,
change `threads_per_chain` option:
rstan_options(threads_per_chain = 1)


Attaching package: ‘rstan’

The following object is masked from ‘package:tidyr’:

    extract

This is bayesplot version 1.11.1
- Online documentation and vignettes at mc-stan.org/bayesplot
- bayesplot theme set to bayesplot::theme_default()
   * Does _not_ affect other ggplot2 plots
   * See ?bayesplot_theme_set for details on theme setting
Rows: 107727 Columns: 26
── Column specification ────────────────────────────────────────────────────────
Delimiter: ","
chr  (3): bw_cond, effect, set
dbl (23): sub_n, block_n, trial_n, distance, diag, h1, w1, h2, w2, h3, w3, a...

ℹ Use `spec()` to retrieve the full column specification for this data.
ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
`summarise()` has grouped output by 'sub_n', 'set', 'distance'. You can
override using the `.groups` argument.
Joining with `by = join_by(sub_n)`
[1] 1
[1] 2
[1] 3
[1] 4
[1] 5
[1] 6
[1] 7
[1] 8
[1] 9
[1] 10
[1] 11
[1] 12
[1] 13
[1] 14
[1] 15
[1] 16
[1] 17
[1] 18
[1] 19
[1] 20
[1] 21
[1] 22
[1] 23
[1] 24
[1] 25
[1] 26
[1] 27
[1] 28
[1] 29
[1] 30
[1] 31
[1] 32
[1] 33
[1] 34
[1] 35
[1] 36
[1] 37
[1] 38
[1] 39
[1] 40
[1] 41
[1] 42
[1] 43
[1] 44
[1] 45
[1] 46
[1] 47
[1] 48
[1] 49
[1] 50
[1] 51
[1] 52
[1] 53
[1] 54
[1] 55
[1] 56
[1] 57
[1] 58
[1] 59
[1] 60
[1] 61
[1] 62
[1] 63
[1] 64
[1] 65
[1] 66
[1] 67
[1] 68
[1] 69
[1] 70
[1] 71
[1] 72
[1] 73
[1] 74
[1] 75
[1] 76
[1] 77
[1] 78
[1] 79
[1] 80
[1] 81
[1] 82
[1] 83
[1] 84
[1] 85
[1] 86
[1] 87
[1] 88
[1] 89
[1] 90
[1] 91
[1] 92
[1] 93
[1] 94
[1] 95
[1] 96
[1] 97
[1] 98
[1] 99
[1] 100
[1] 101
[1] 102
[1] 103
[1] 104
[1] 105
[1] 106
[1] 107
[1] 108
[1] 109
[1] 110
[1] 111
[1] 112
[1] 113
[1] 114
[1] 115
[1] 116
[1] 117
[1] 118
[1] 119
[1] 120
[1] 121
[1] 122
[1] 123
[1] 124
[1] 125
[1] 126
[1] 127
[1] 128
[1] 129
[1] 130
[1] 131
[1] 132
[1] 133
[1] 134
[1] 135
[1] 136
[1] 137
[1] 138
[1] 139
[1] 140
[1] 141
[1] 142
[1] 143
[1] 144
[1] 145
[1] 146
[1] 147
[1] 148
[1] 149
[1] 150
[1] 151
[1] 152
[1] 153
[1] 154
[1] 155
[1] 156
[1] 157
[1] 158
[1] 159
[1] 160
[1] 161
[1] 162
[1] 163
[1] 164
[1] 165
[1] 166
[1] 167
[1] 168
[1] 169
[1] 170
[1] 171
[1] 172
[1] 173
[1] 174
[1] 175
[1] 176
[1] 177
[1] 178
[1] 179
[1] 180
[1] 181
[1] 182
[1] 183
[1] 184
[1] 185
[1] 186
[1] 187
[1] 188
[1] 189
[1] 190
[1] 191
[1] 192
[1] 193
[1] 194
[1] 195
[1] 196
[1] 197
[1] 198
[1] 199
[1] 200
[1] 201
[1] 202
[1] 203
[1] 204
[1] 205
[1] 206
[1] 207
[1] 208
[1] 209
[1] 210
[1] 211
[1] 212
[1] 213
[1] 214
[1] 215
[1] 216
[1] 217
[1] 218
[1] 219
[1] 220
[1] 221
[1] 222
[1] 223
[1] 224
[1] 225
[1] 226
[1] 227
[1] 228
[1] 229
[1] 230
[1] 231
[1] 232
[1] 233
[1] 234
[1] 235
[1] 236
[1] 237
[1] 238
[1] 239
[1] 240
[1] 241
[1] 242
[1] 243
[1] 244
[1] 245
[1] 246
[1] 247
[1] 248
[1] 249
[1] 250
[1] 251
[1] 252
[1] 253
[1] 254
[1] 255
[1] 256
[1] 257
[1] 258
[1] 259
[1] 260
[1] 261
[1] 262
[1] 263
[1] 264
[1] 265
[1] 266
[1] 267
[1] 268
[1] 269
[1] 270
[1] 271
[1] 272
[1] 273
[1] 274
[1] 275
[1] 276
[1] 277
[1] 278
[1] 279
[1] 280
[1] 281
[1] 282
[1] 283
[1] 284
[1] 285
[1] 286
[1] 287
[1] 288
[1] 289
[1] 290
[1] 291
[1] 292
[1] 293
[1] 294
[1] 295
[1] 296
[1] 297
[1] 298
[1] 299
[1] 300
[1] 301
[1] 302
[1] 303
[1] 304
[1] 305
[1] 306
[1] 307
[1] 308
[1] 309
[1] 310
[1] 311
[1] 312
[1] 313
[1] 314
[1] 315
[1] 316
[1] 317
[1] 318
[1] 319
[1] 320
[1] 321
[1] 322
[1] 323
[1] 324
[1] 325
[1] 326
[1] 327
[1] 328
[1] 329
[1] 330
[1] 331
[1] 332
[1] 333
[1] 334
[1] 335
[1] 336
[1] 337
[1] 338
[1] 339
[1] 340
[1] 341
[1] 342
[1] 343
[1] 344
[1] 345
[1] 346
[1] 347
[1] 348
[1] 349
[1] 350
[1] 351
[1] 352
[1] 353
[1] 354
[1] 355
[1] 356
[1] 357
[1] 358
[1] 359
[1] 360
[1] 361
[1] 362
[1] 363
[1] 364
[1] 365
[1] 366
[1] 367
[1] 368
[1] 369

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).
Chain 1: 
Chain 1: Gradient evaluation took 0.007961 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 79.61 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 3: 
Chain 3: Gradient evaluation took 0.007887 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 78.87 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 2: 
Chain 2: Gradient evaluation took 0.007965 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 79.65 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 4: 
Chain 4: Gradient evaluation took 0.00789 seconds
Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 78.9 seconds.
Chain 4: Adjust your expectations accordingly!
Chain 4: 
Chain 4: 
Chain 1: Iteration:   1 / 1000 [  0%]  (Warmup)
Chain 2: Iteration:   1 / 1000 [  0%]  (Warmup)
Chain 3: Iteration:   1 / 1000 [  0%]  (Warmup)
Chain 4: Iteration:   1 / 1000 [  0%]  (Warmup)
Chain 1: Iteration: 100 / 1000 [ 10%]  (Warmup)
Chain 2: Iteration: 100 / 1000 [ 10%]  (Warmup)
Chain 4: Iteration: 100 / 1000 [ 10%]  (Warmup)
Chain 3: Iteration: 100 / 1000 [ 10%]  (Warmup)
Chain 1: Iteration: 200 / 1000 [ 20%]  (Warmup)
Chain 2: Iteration: 200 / 1000 [ 20%]  (Warmup)
Chain 4: Iteration: 200 / 1000 [ 20%]  (Warmup)
Chain 1: Iteration: 300 / 1000 [ 30%]  (Warmup)
Chain 3: Iteration: 200 / 1000 [ 20%]  (Warmup)
Chain 2: Iteration: 300 / 1000 [ 30%]  (Warmup)
Chain 4: Iteration: 300 / 1000 [ 30%]  (Warmup)
Chain 1: Iteration: 400 / 1000 [ 40%]  (Warmup)
Chain 3: Iteration: 300 / 1000 [ 30%]  (Warmup)
Chain 4: Iteration: 400 / 1000 [ 40%]  (Warmup)
Chain 2: Iteration: 400 / 1000 [ 40%]  (Warmup)
Chain 1: Iteration: 500 / 1000 [ 50%]  (Warmup)
Chain 1: Iteration: 501 / 1000 [ 50%]  (Sampling)
Chain 3: Iteration: 400 / 1000 [ 40%]  (Warmup)
Chain 4: Iteration: 500 / 1000 [ 50%]  (Warmup)
Chain 4: Iteration: 501 / 1000 [ 50%]  (Sampling)
Chain 2: Iteration: 500 / 1000 [ 50%]  (Warmup)
Chain 2: Iteration: 501 / 1000 [ 50%]  (Sampling)
Chain 1: Iteration: 600 / 1000 [ 60%]  (Sampling)
Chain 3: Iteration: 500 / 1000 [ 50%]  (Warmup)
Chain 3: Iteration: 501 / 1000 [ 50%]  (Sampling)
Chain 4: Iteration: 600 / 1000 [ 60%]  (Sampling)
Chain 2: Iteration: 600 / 1000 [ 60%]  (Sampling)
Chain 1: Iteration: 700 / 1000 [ 70%]  (Sampling)
Chain 3: Iteration: 600 / 1000 [ 60%]  (Sampling)
Chain 4: Iteration: 700 / 1000 [ 70%]  (Sampling)
Chain 2: Iteration: 700 / 1000 [ 70%]  (Sampling)
Chain 1: Iteration: 800 / 1000 [ 80%]  (Sampling)
Chain 3: Iteration: 700 / 1000 [ 70%]  (Sampling)
Chain 4: Iteration: 800 / 1000 [ 80%]  (Sampling)
Chain 2: Iteration: 800 / 1000 [ 80%]  (Sampling)
Chain 1: Iteration: 900 / 1000 [ 90%]  (Sampling)
Chain 3: Iteration: 800 / 1000 [ 80%]  (Sampling)
Chain 4: Iteration: 900 / 1000 [ 90%]  (Sampling)
Chain 2: Iteration: 900 / 1000 [ 90%]  (Sampling)
Chain 1: Iteration: 1000 / 1000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 324.307 seconds (Warm-up)
Chain 1:                85.226 seconds (Sampling)
Chain 1:                409.533 seconds (Total)
Chain 1: 
Chain 3: Iteration: 900 / 1000 [ 90%]  (Sampling)
Chain 4: Iteration: 1000 / 1000 [100%]  (Sampling)
Chain 4: 
Chain 4:  Elapsed Time: 338.775 seconds (Warm-up)
Chain 4:                85.623 seconds (Sampling)
Chain 4:                424.398 seconds (Total)
Chain 4: 
Chain 2: Iteration: 1000 / 1000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 339.255 seconds (Warm-up)
Chain 2:                86.888 seconds (Sampling)
Chain 2:                426.143 seconds (Total)
Chain 2: 
Chain 3: Iteration: 1000 / 1000 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 352.056 seconds (Warm-up)
Chain 3:                85.062 seconds (Sampling)
Chain 3:                437.118 seconds (Total)
Chain 3: 
Warning message:
Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
Running the chains for more iterations may help. See
https://mc-stan.org/misc/warnings.html#bulk-ess 
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
 0.9981  0.9990  0.9995  0.9997  1.0002  1.0287 
[1] 1
[1] 2
[1] 3
[1] 4
[1] 1
[1] 1
[1] 2
[1] 3
[1] 4
[1] 2
[1] 1
[1] 2
[1] 3
[1] 4
[1] 3
[1] 1
[1] 2
[1] 3
[1] 4
[1] 4
[1] 1
[1] 2
[1] 3
[1] 4
[1] 5
[1] 1
[1] 2
[1] 3
[1] 4
[1] 6
[1] 1
[1] 2
[1] 3
[1] 4
[1] 7
[1] 1
[1] 2
[1] 3
[1] 4
[1] 8
[1] 1
[1] 2
[1] 3
[1] 4
[1] 9
[1] 1
[1] 2
[1] 3
[1] 4
[1] 10
[1] 1
[1] 2
[1] 3
[1] 4
[1] 11
[1] 1
[1] 2
[1] 3
[1] 4
[1] 12
[1] 1
[1] 2
[1] 3
[1] 4
[1] 13
[1] 1
[1] 2
[1] 3
[1] 4
[1] 14
[1] 1
[1] 2
[1] 3
[1] 4
[1] 15
[1] 1
[1] 2
[1] 3
[1] 4
[1] 16
[1] 1
[1] 2
[1] 3
[1] 4
[1] 17
[1] 1
[1] 2
[1] 3
[1] 4
[1] 18
[1] 1
[1] 2
[1] 3
[1] 4
[1] 19
[1] 1
[1] 2
[1] 3
[1] 4
[1] 20
[1] 1
[1] 2
[1] 3
[1] 4
[1] 21
[1] 1
[1] 2
[1] 3
[1] 4
[1] 22
[1] 1
[1] 2
[1] 3
[1] 4
[1] 23
[1] 1
[1] 2
[1] 3
[1] 4
[1] 24
[1] 1
[1] 2
[1] 3
[1] 4
[1] 25
[1] 1
[1] 2
[1] 3
[1] 4
[1] 26
[1] 1
[1] 2
[1] 3
[1] 4
[1] 27
[1] 1
[1] 2
[1] 3
[1] 4
[1] 28
[1] 1
[1] 2
[1] 3
[1] 4
[1] 29
[1] 1
[1] 2
[1] 3
[1] 4
[1] 30
[1] 1
[1] 2
[1] 3
[1] 4
[1] 31
[1] 1
[1] 2
[1] 3
[1] 4
[1] 32
[1] 1
[1] 2
[1] 3
[1] 4
[1] 33
[1] 1
[1] 2
[1] 3
[1] 4
[1] 34
[1] 1
[1] 2
[1] 3
[1] 4
[1] 35
[1] 1
[1] 2
[1] 3
[1] 4
[1] 36
[1] 1
[1] 2
[1] 3
[1] 4
[1] 37
[1] 1
[1] 2
[1] 3
[1] 4
[1] 38
[1] 1
[1] 2
[1] 3
[1] 4
[1] 39
[1] 1
[1] 2
[1] 3
[1] 4
[1] 40
[1] 1
[1] 2
[1] 3
[1] 4
[1] 41
[1] 1
[1] 2
[1] 3
[1] 4
[1] 42
[1] 1
[1] 2
[1] 3
[1] 4
[1] 43
[1] 1
[1] 2
[1] 3
[1] 4
[1] 44
[1] 1
[1] 2
[1] 3
[1] 4
[1] 45
[1] 1
[1] 2
[1] 3
[1] 4
[1] 46
[1] 1
[1] 2
[1] 3
[1] 4
[1] 47
[1] 1
[1] 2
[1] 3
[1] 4
[1] 48
[1] 1
[1] 2
[1] 3
[1] 4
[1] 49
[1] 1
[1] 2
[1] 3
[1] 4
[1] 50
[1] 1
[1] 2
[1] 3
[1] 4
[1] 51
[1] 1
[1] 2
[1] 3
[1] 4
[1] 52
[1] 1
[1] 2
[1] 3
[1] 4
[1] 53
[1] 1
[1] 2
[1] 3
[1] 4
[1] 54
[1] 1
[1] 2
[1] 3
[1] 4
[1] 55
[1] 1
[1] 2
[1] 3
[1] 4
[1] 56
[1] 1
[1] 2
[1] 3
[1] 4
[1] 57
[1] 1
[1] 2
[1] 3
[1] 4
[1] 58
[1] 1
[1] 2
[1] 3
[1] 4
[1] 59
[1] 1
[1] 2
[1] 3
[1] 4
[1] 60
[1] 1
[1] 2
[1] 3
[1] 4
[1] 61
[1] 1
[1] 2
[1] 3
[1] 4
[1] 62
[1] 1
[1] 2
[1] 3
[1] 4
[1] 63
[1] 1
[1] 2
[1] 3
[1] 4
[1] 64
[1] 1
[1] 2
[1] 3
[1] 4
[1] 65
[1] 1
[1] 2
[1] 3
[1] 4
[1] 66
[1] 1
[1] 2
[1] 3
[1] 4
[1] 67
[1] 1
[1] 2
[1] 3
[1] 4
[1] 68
[1] 1
[1] 2
[1] 3
[1] 4
[1] 69
[1] 1
[1] 2
[1] 3
[1] 4
[1] 70
[1] 1
[1] 2
[1] 3
[1] 4
[1] 71
[1] 1
[1] 2
[1] 3
[1] 4
[1] 72
[1] 1
[1] 2
[1] 3
[1] 4
[1] 73
[1] 1
[1] 2
[1] 3
[1] 4
[1] 74
[1] 1
[1] 2
[1] 3
[1] 4
[1] 75
[1] 1
[1] 2
[1] 3
[1] 4
[1] 76
[1] 1
[1] 2
[1] 3
[1] 4
[1] 77
[1] 1
[1] 2
[1] 3
[1] 4
[1] 78
[1] 1
[1] 2
[1] 3
[1] 4
[1] 79
[1] 1
[1] 2
[1] 3
[1] 4
[1] 80
[1] 1
[1] 2
[1] 3
[1] 4
[1] 81
[1] 1
[1] 2
[1] 3
[1] 4
[1] 82
[1] 1
[1] 2
[1] 3
[1] 4
[1] 83
[1] 1
[1] 2
[1] 3
[1] 4
[1] 84
[1] 1
[1] 2
[1] 3
[1] 4
[1] 85
[1] 1
[1] 2
[1] 3
[1] 4
[1] 86
[1] 1
[1] 2
[1] 3
[1] 4
[1] 87
[1] 1
[1] 2
[1] 3
[1] 4
[1] 88
[1] 1
[1] 2
[1] 3
[1] 4
[1] 89
[1] 1
[1] 2
[1] 3
[1] 4
[1] 90
[1] 1
[1] 2
[1] 3
[1] 4
[1] 91
[1] 1
[1] 2
[1] 3
[1] 4
[1] 92
[1] 1
[1] 2
[1] 3
[1] 4
[1] 93
[1] 1
[1] 2
[1] 3
[1] 4
[1] 94
[1] 1
[1] 2
[1] 3
[1] 4
[1] 95
[1] 1
[1] 2
[1] 3
[1] 4
[1] 96
[1] 1
[1] 2
[1] 3
[1] 4
[1] 97
[1] 1
[1] 2
[1] 3
[1] 4
[1] 98
[1] 1
[1] 2
[1] 3
[1] 4
[1] 99
[1] 1
[1] 2
[1] 3
[1] 4
[1] 100
[1] 1
[1] 2
[1] 3
[1] 4
[1] 101
[1] 1
[1] 2
[1] 3
[1] 4
[1] 102
[1] 1
[1] 2
[1] 3
[1] 4
[1] 103
[1] 1
[1] 2
[1] 3
[1] 4
[1] 104
[1] 1
[1] 2
[1] 3
[1] 4
[1] 105
[1] 1
[1] 2
[1] 3
[1] 4
[1] 106
[1] 1
[1] 2
[1] 3
[1] 4
[1] 107
[1] 1
[1] 2
[1] 3
[1] 4
[1] 108
[1] 1
[1] 2
[1] 3
[1] 4
[1] 109
[1] 1
[1] 2
[1] 3
[1] 4
[1] 110
[1] 1
[1] 2
[1] 3
[1] 4
[1] 111
[1] 1
[1] 2
[1] 3
[1] 4
[1] 112
[1] 1
[1] 2
[1] 3
[1] 4
[1] 113
[1] 1
[1] 2
[1] 3
[1] 4
[1] 114
[1] 1
[1] 2
[1] 3
[1] 4
[1] 115
[1] 1
[1] 2
[1] 3
[1] 4
[1] 116
[1] 1
[1] 2
[1] 3
[1] 4
[1] 117
[1] 1
[1] 2
[1] 3
[1] 4
[1] 118
[1] 1
[1] 2
[1] 3
[1] 4
[1] 119
[1] 1
[1] 2
[1] 3
[1] 4
[1] 120
[1] 1
[1] 2
[1] 3
[1] 4
[1] 121
[1] 1
[1] 2
[1] 3
[1] 4
[1] 122
[1] 1
[1] 2
[1] 3
[1] 4
[1] 123
[1] 1
[1] 2
[1] 3
[1] 4
[1] 124
[1] 1
[1] 2
[1] 3
[1] 4
[1] 125
[1] 1
[1] 2
[1] 3
[1] 4
[1] 126
[1] 1
[1] 2
[1] 3
[1] 4
[1] 127
[1] 1
[1] 2
[1] 3
[1] 4
[1] 128
[1] 1
[1] 2
[1] 3
[1] 4
[1] 129
[1] 1
[1] 2
[1] 3
[1] 4
[1] 130
[1] 1
[1] 2
[1] 3
[1] 4
[1] 131
[1] 1
[1] 2
[1] 3
[1] 4
[1] 132
[1] 1
[1] 2
[1] 3
[1] 4
[1] 133
[1] 1
[1] 2
[1] 3
[1] 4
[1] 134
[1] 1
[1] 2
[1] 3
[1] 4
[1] 135
[1] 1
[1] 2
[1] 3
[1] 4
[1] 136
[1] 1
[1] 2
[1] 3
[1] 4
[1] 137
[1] 1
[1] 2
[1] 3
[1] 4
[1] 138
[1] 1
[1] 2
[1] 3
[1] 4
[1] 139
[1] 1
[1] 2
[1] 3
[1] 4
[1] 140
[1] 1
[1] 2
[1] 3
[1] 4
[1] 141
[1] 1
[1] 2
[1] 3
[1] 4
[1] 142
[1] 1
[1] 2
[1] 3
[1] 4
[1] 143
[1] 1
[1] 2
[1] 3
[1] 4
[1] 144
[1] 1
[1] 2
[1] 3
[1] 4
[1] 145
[1] 1
[1] 2
[1] 3
[1] 4
[1] 146
[1] 1
[1] 2
[1] 3
[1] 4
[1] 147
[1] 1
[1] 2
[1] 3
[1] 4
[1] 148
[1] 1
[1] 2
[1] 3
[1] 4
[1] 149
[1] 1
[1] 2
[1] 3
[1] 4
[1] 150
[1] 1
[1] 2
[1] 3
[1] 4
[1] 151
[1] 1
[1] 2
[1] 3
[1] 4
[1] 152
[1] 1
[1] 2
[1] 3
[1] 4
[1] 153
[1] 1
[1] 2
[1] 3
[1] 4
[1] 154
[1] 1
[1] 2
[1] 3
[1] 4
[1] 155
[1] 1
[1] 2
[1] 3
[1] 4
[1] 156
[1] 1
[1] 2
[1] 3
[1] 4
[1] 157
[1] 1
[1] 2
[1] 3
[1] 4
[1] 158
[1] 1
[1] 2
[1] 3
[1] 4
[1] 159
[1] 1
[1] 2
[1] 3
[1] 4
[1] 160
[1] 1
[1] 2
[1] 3
[1] 4
[1] 161
[1] 1
[1] 2
[1] 3
[1] 4
[1] 162
[1] 1
[1] 2
[1] 3
[1] 4
[1] 163
[1] 1
[1] 2
[1] 3
[1] 4
[1] 164
[1] 1
[1] 2
[1] 3
[1] 4
[1] 165
[1] 1
[1] 2
[1] 3
[1] 4
[1] 166
[1] 1
[1] 2
[1] 3
[1] 4
[1] 167
[1] 1
[1] 2
[1] 3
[1] 4
[1] 168
[1] 1
[1] 2
[1] 3
[1] 4
[1] 169
[1] 1
[1] 2
[1] 3
[1] 4
[1] 170
[1] 1
[1] 2
[1] 3
[1] 4
[1] 171
[1] 1
[1] 2
[1] 3
[1] 4
[1] 172
[1] 1
[1] 2
[1] 3
[1] 4
[1] 173
[1] 1
[1] 2
[1] 3
[1] 4
[1] 174
[1] 1
[1] 2
[1] 3
[1] 4
[1] 175
[1] 1
[1] 2
[1] 3
[1] 4
[1] 176
[1] 1
[1] 2
[1] 3
[1] 4
[1] 177
[1] 1
[1] 2
[1] 3
[1] 4
[1] 178
[1] 1
[1] 2
[1] 3
[1] 4
[1] 179
[1] 1
[1] 2
[1] 3
[1] 4
[1] 180
[1] 1
[1] 2
[1] 3
[1] 4
[1] 181
[1] 1
[1] 2
[1] 3
[1] 4
[1] 182
[1] 1
[1] 2
[1] 3
[1] 4
[1] 183
[1] 1
[1] 2
[1] 3
[1] 4
[1] 184
[1] 1
[1] 2
[1] 3
[1] 4
[1] 185
[1] 1
[1] 2
[1] 3
[1] 4
[1] 186
[1] 1
[1] 2
[1] 3
[1] 4
[1] 187
[1] 1
[1] 2
[1] 3
[1] 4
[1] 188
[1] 1
[1] 2
[1] 3
[1] 4
[1] 189
[1] 1
[1] 2
[1] 3
[1] 4
[1] 190
[1] 1
[1] 2
[1] 3
[1] 4
[1] 191
[1] 1
[1] 2
[1] 3
[1] 4
[1] 192
[1] 1
[1] 2
[1] 3
[1] 4
[1] 193
[1] 1
[1] 2
[1] 3
[1] 4
[1] 194
[1] 1
[1] 2
[1] 3
[1] 4
[1] 195
[1] 1
[1] 2
[1] 3
[1] 4
[1] 196
[1] 1
[1] 2
[1] 3
[1] 4
[1] 197
[1] 1
[1] 2
[1] 3
[1] 4
[1] 198
[1] 1
[1] 2
[1] 3
[1] 4
[1] 199
[1] 1
[1] 2
[1] 3
[1] 4
[1] 200
[1] 1
[1] 2
[1] 3
[1] 4
[1] 201
[1] 1
[1] 2
[1] 3
[1] 4
[1] 202
[1] 1
[1] 2
[1] 3
[1] 4
[1] 203
[1] 1
[1] 2
[1] 3
[1] 4
[1] 204
[1] 1
[1] 2
[1] 3
[1] 4
[1] 205
[1] 1
[1] 2
[1] 3
[1] 4
[1] 206
[1] 1
[1] 2
[1] 3
[1] 4
[1] 207
[1] 1
[1] 2
[1] 3
[1] 4
[1] 208
[1] 1
[1] 2
[1] 3
[1] 4
[1] 209
[1] 1
[1] 2
[1] 3
[1] 4
[1] 210
[1] 1
[1] 2
[1] 3
[1] 4
[1] 211
[1] 1
[1] 2
[1] 3
[1] 4
[1] 212
[1] 1
[1] 2
[1] 3
[1] 4
[1] 213
[1] 1
[1] 2
[1] 3
[1] 4
[1] 214
[1] 1
[1] 2
[1] 3
[1] 4
[1] 215
[1] 1
[1] 2
[1] 3
[1] 4
[1] 216
[1] 1
[1] 2
[1] 3
[1] 4
[1] 217
[1] 1
[1] 2
[1] 3
[1] 4
[1] 218
[1] 1
[1] 2
[1] 3
[1] 4
[1] 219
[1] 1
[1] 2
[1] 3
[1] 4
[1] 220
[1] 1
[1] 2
[1] 3
[1] 4
[1] 221
[1] 1
[1] 2
[1] 3
[1] 4
[1] 222
[1] 1
[1] 2
[1] 3
[1] 4
[1] 223
[1] 1
[1] 2
[1] 3
[1] 4
[1] 224
[1] 1
[1] 2
[1] 3
[1] 4
[1] 225
[1] 1
[1] 2
[1] 3
[1] 4
[1] 226
[1] 1
[1] 2
[1] 3
[1] 4
[1] 227
[1] 1
[1] 2
[1] 3
[1] 4
[1] 228
[1] 1
[1] 2
[1] 3
[1] 4
[1] 229
[1] 1
[1] 2
[1] 3
[1] 4
[1] 230
[1] 1
[1] 2
[1] 3
[1] 4
[1] 231
[1] 1
[1] 2
[1] 3
[1] 4
[1] 232
[1] 1
[1] 2
[1] 3
[1] 4
[1] 233
[1] 1
[1] 2
[1] 3
[1] 4
[1] 234
[1] 1
[1] 2
[1] 3
[1] 4
[1] 235
[1] 1
[1] 2
[1] 3
[1] 4
[1] 236
[1] 1
[1] 2
[1] 3
[1] 4
[1] 237
[1] 1
[1] 2
[1] 3
[1] 4
[1] 238
[1] 1
[1] 2
[1] 3
[1] 4
[1] 239
[1] 1
[1] 2
[1] 3
[1] 4
[1] 240
[1] 1
[1] 2
[1] 3
[1] 4
[1] 241
[1] 1
[1] 2
[1] 3
[1] 4
[1] 242
[1] 1
[1] 2
[1] 3
[1] 4
[1] 243
[1] 1
[1] 2
[1] 3
[1] 4
[1] 244
[1] 1
[1] 2
[1] 3
[1] 4
[1] 245
[1] 1
[1] 2
[1] 3
[1] 4
[1] 246
[1] 1
[1] 2
[1] 3
[1] 4
[1] 247
[1] 1
[1] 2
[1] 3
[1] 4
[1] 248
[1] 1
[1] 2
[1] 3
[1] 4
[1] 249
[1] 1
[1] 2
[1] 3
[1] 4
[1] 250
[1] 1
[1] 2
[1] 3
[1] 4
[1] 251
[1] 1
[1] 2
[1] 3
[1] 4
[1] 252
[1] 1
[1] 2
[1] 3
[1] 4
[1] 253
[1] 1
[1] 2
[1] 3
[1] 4
[1] 254
[1] 1
[1] 2
[1] 3
[1] 4
[1] 255
[1] 1
[1] 2
[1] 3
[1] 4
[1] 256
[1] 1
[1] 2
[1] 3
[1] 4
[1] 257
[1] 1
[1] 2
[1] 3
[1] 4
[1] 258
[1] 1
[1] 2
[1] 3
[1] 4
[1] 259
[1] 1
[1] 2
[1] 3
[1] 4
[1] 260
[1] 1
[1] 2
[1] 3
[1] 4
[1] 261
[1] 1
[1] 2
[1] 3
[1] 4
[1] 262
[1] 1
[1] 2
[1] 3
[1] 4
[1] 263
[1] 1
[1] 2
[1] 3
[1] 4
[1] 264
[1] 1
[1] 2
[1] 3
[1] 4
[1] 265
[1] 1
[1] 2
[1] 3
[1] 4
[1] 266
[1] 1
[1] 2
[1] 3
[1] 4
[1] 267
[1] 1
[1] 2
[1] 3
[1] 4
[1] 268
[1] 1
[1] 2
[1] 3
[1] 4
[1] 269
[1] 1
[1] 2
[1] 3
[1] 4
[1] 270
[1] 1
[1] 2
[1] 3
[1] 4
[1] 271
[1] 1
[1] 2
[1] 3
[1] 4
[1] 272
[1] 1
[1] 2
[1] 3
[1] 4
[1] 273
[1] 1
[1] 2
[1] 3
[1] 4
[1] 274
[1] 1
[1] 2
[1] 3
[1] 4
[1] 275
[1] 1
[1] 2
[1] 3
[1] 4
[1] 276
[1] 1
[1] 2
[1] 3
[1] 4
[1] 277
[1] 1
[1] 2
[1] 3
[1] 4
[1] 278
[1] 1
[1] 2
[1] 3
[1] 4
[1] 279
[1] 1
[1] 2
[1] 3
[1] 4
[1] 280
[1] 1
[1] 2
[1] 3
[1] 4
[1] 281
[1] 1
[1] 2
[1] 3
[1] 4
[1] 282
[1] 1
[1] 2
[1] 3
[1] 4
[1] 283
[1] 1
[1] 2
[1] 3
[1] 4
[1] 284
[1] 1
[1] 2
[1] 3
[1] 4
[1] 285
[1] 1
[1] 2
[1] 3
[1] 4
[1] 286
[1] 1
[1] 2
[1] 3
[1] 4
[1] 287
[1] 1
[1] 2
[1] 3
[1] 4
[1] 288
[1] 1
[1] 2
[1] 3
[1] 4
[1] 289
[1] 1
[1] 2
[1] 3
[1] 4
[1] 290
[1] 1
[1] 2
[1] 3
[1] 4
[1] 291
[1] 1
[1] 2
[1] 3
[1] 4
[1] 292
[1] 1
[1] 2
[1] 3
[1] 4
[1] 293
[1] 1
[1] 2
[1] 3
[1] 4
[1] 294
[1] 1
[1] 2
[1] 3
[1] 4
[1] 295
[1] 1
[1] 2
[1] 3
[1] 4
[1] 296
[1] 1
[1] 2
[1] 3
[1] 4
[1] 297
[1] 1
[1] 2
[1] 3
[1] 4
[1] 298
[1] 1
[1] 2
[1] 3
[1] 4
[1] 299
[1] 1
[1] 2
[1] 3
[1] 4
[1] 300
[1] 1
[1] 2
[1] 3
[1] 4
[1] 301
[1] 1
[1] 2
[1] 3
[1] 4
[1] 302
[1] 1
[1] 2
[1] 3
[1] 4
[1] 303
[1] 1
[1] 2
[1] 3
[1] 4
[1] 304
[1] 1
[1] 2
[1] 3
[1] 4
[1] 305
[1] 1
[1] 2
[1] 3
[1] 4
[1] 306
[1] 1
[1] 2
[1] 3
[1] 4
[1] 307
[1] 1
[1] 2
[1] 3
[1] 4
[1] 308
[1] 1
[1] 2
[1] 3
[1] 4
[1] 309
[1] 1
[1] 2
[1] 3
[1] 4
[1] 310
[1] 1
[1] 2
[1] 3
[1] 4
[1] 311
[1] 1
[1] 2
[1] 3
[1] 4
[1] 312
[1] 1
[1] 2
[1] 3
[1] 4
[1] 313
[1] 1
[1] 2
[1] 3
[1] 4
[1] 314
[1] 1
[1] 2
[1] 3
[1] 4
[1] 315
[1] 1
[1] 2
[1] 3
[1] 4
[1] 316
[1] 1
[1] 2
[1] 3
[1] 4
[1] 317
[1] 1
[1] 2
[1] 3
[1] 4
[1] 318
[1] 1
[1] 2
[1] 3
[1] 4
[1] 319
[1] 1
[1] 2
[1] 3
[1] 4
[1] 320
[1] 1
[1] 2
[1] 3
[1] 4
[1] 321
[1] 1
[1] 2
[1] 3
[1] 4
[1] 322
[1] 1
[1] 2
[1] 3
[1] 4
[1] 323
[1] 1
[1] 2
[1] 3
[1] 4
[1] 324
[1] 1
[1] 2
[1] 3
[1] 4
[1] 325
[1] 1
[1] 2
[1] 3
[1] 4
[1] 326
[1] 1
[1] 2
[1] 3
[1] 4
[1] 327
[1] 1
[1] 2
[1] 3
[1] 4
[1] 328
[1] 1
[1] 2
[1] 3
[1] 4
[1] 329
[1] 1
[1] 2
[1] 3
[1] 4
[1] 330
[1] 1
[1] 2
[1] 3
[1] 4
[1] 331
[1] 1
[1] 2
[1] 3
[1] 4
[1] 332
[1] 1
[1] 2
[1] 3
[1] 4
[1] 333
[1] 1
[1] 2
[1] 3
[1] 4
[1] 334
[1] 1
[1] 2
[1] 3
[1] 4
[1] 335
[1] 1
[1] 2
[1] 3
[1] 4
[1] 336
[1] 1
[1] 2
[1] 3
[1] 4
[1] 337
[1] 1
[1] 2
[1] 3
[1] 4
[1] 338
[1] 1
[1] 2
[1] 3
[1] 4
[1] 339
[1] 1
[1] 2
[1] 3
[1] 4
[1] 340
[1] 1
[1] 2
[1] 3
[1] 4
[1] 341
[1] 1
[1] 2
[1] 3
[1] 4
[1] 342
[1] 1
[1] 2
[1] 3
[1] 4
[1] 343
[1] 1
[1] 2
[1] 3
[1] 4
[1] 344
[1] 1
[1] 2
[1] 3
[1] 4
[1] 345
[1] 1
[1] 2
[1] 3
[1] 4
[1] 346
[1] 1
[1] 2
[1] 3
[1] 4
[1] 347
[1] 1
[1] 2
[1] 3
[1] 4
[1] 348
[1] 1
[1] 2
[1] 3
[1] 4
[1] 349
[1] 1
[1] 2
[1] 3
[1] 4
[1] 350
[1] 1
[1] 2
[1] 3
[1] 4
[1] 351
[1] 1
[1] 2
[1] 3
[1] 4
[1] 352
[1] 1
[1] 2
[1] 3
[1] 4
[1] 353
[1] 1
[1] 2
[1] 3
[1] 4
[1] 354
[1] 1
[1] 2
[1] 3
[1] 4
[1] 355
[1] 1
[1] 2
[1] 3
[1] 4
[1] 356
[1] 1
[1] 2
[1] 3
[1] 4
[1] 357
[1] 1
[1] 2
[1] 3
[1] 4
[1] 358
[1] 1
[1] 2
[1] 3
[1] 4
[1] 359
[1] 1
[1] 2
[1] 3
[1] 4
[1] 360
[1] 1
[1] 2
[1] 3
[1] 4
[1] 361
[1] 1
[1] 2
[1] 3
[1] 4
[1] 362
[1] 1
[1] 2
[1] 3
[1] 4
[1] 363
[1] 1
[1] 2
[1] 3
[1] 4
[1] 364
[1] 1
[1] 2
[1] 3
[1] 4
[1] 365
[1] 1
[1] 2
[1] 3
[1] 4
[1] 366
[1] 1
[1] 2
[1] 3
[1] 4
[1] 367
[1] 1
[1] 2
[1] 3
[1] 4
[1] 368
[1] 1
[1] 2
[1] 3
[1] 4
[1] 369
[1] 1
[1] 2
[1] 3
[1] 4
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
# A tibble: 4,428 × 9
   sub_n distance option p_best_m p_best_m_l p_best_m_u p_worst_m p_worst_m_l
   <int>    <dbl> <chr>     <dbl>      <dbl>      <dbl>     <dbl>       <dbl>
 1     1        2 t         0.286     0.200       0.364     0.264      0.187 
 2     1        2 c         0.436     0.336       0.522     0.370      0.279 
 3     1        2 d         0.278     0.197       0.367     0.365      0.281 
 4     1        5 t         0.365     0.279       0.455     0.195      0.118 
 5     1        5 c         0.469     0.373       0.565     0.222      0.140 
 6     1        5 d         0.166     0.102       0.246     0.584      0.489 
 7     1        9 t         0.380     0.281       0.480     0.181      0.104 
 8     1        9 c         0.475     0.371       0.583     0.209      0.111 
 9     1        9 d         0.145     0.0722      0.219     0.611      0.507 
10     1       14 t         0.437     0.337       0.536     0.101      0.0449
# ℹ 4,418 more rows
# ℹ 1 more variable: p_worst_m_u <dbl>
# A tibble: 68,673 × 31
   sub_n bw_cond block_n trial_n effect   set   distance  diag    h1    w1    h2
   <dbl> <chr>     <dbl>   <dbl> <chr>    <chr>    <dbl> <dbl> <dbl> <dbl> <dbl>
 1    22 wb            1       6 attract… h            9     2    90   165   165
 2    22 wb            1       8 attract… h            5     2   161    88    90
 3    22 wb            1       9 attract… w            5     2   165    90    90
 4    22 wb            1      12 attract… h            5     3   195   120   120
 5    22 wb            1      13 attract… h            2     2    90   165   163
 6    22 wb            1      14 attract… w            9     2    86   157    90
 7    22 wb            1      15 attract… h            2     1    60   135   135
 8    22 wb            1      16 attract… h            2     3   120   195   195
 9    22 wb            1      17 attract… h            5     1    60   135   132
10    22 wb            1      18 attract… w            5     3   195   120   120
# ℹ 68,663 more rows
# ℹ 20 more variables: w2 <dbl>, h3 <dbl>, w3 <dbl>, a1 <dbl>, a2 <dbl>,
#   a3 <dbl>, choice_best <dbl>, choice_worst <dbl>, rt_best <dbl>,
#   rt_worst <dbl>, a_max <dbl>, a_min <dbl>, choice_best_correct <dbl>,
#   choice_worst_correct <dbl>, both_correct <dbl>, min <int>, best <chr>,
#   worst <chr>, best_att <chr>, worst_att <chr>
`summarise()` has grouped output by 'distance', 'type', 'option'. You can
override using the `.groups` argument.
Joining with `by = join_by(sub_n)`
Warning message:
Removed 93 rows containing missing values or values outside the scale range
(`geom_point()`). 
Warning message:
Removed 93 rows containing missing values or values outside the scale range
(`geom_point()`). 
Warning message:
Removed 38 rows containing missing values or values outside the scale range
(`geom_point()`). 
Warning message:
Removed 38 rows containing missing values or values outside the scale range
(`geom_point()`). 
`summarise()` has grouped output by 'distance', 'option'. You can override
using the `.groups` argument.
[1] 0.1917678
`summarise()` has grouped output by 'distance'. You can override using the
`.groups` argument.

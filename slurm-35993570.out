Loading r-rocker-ml-verse version 4.4.0+apptainer
Loading apptainer version latest

No apptainer cache directory found. To prevent apptainer from filling up your
home directory, you can create a new directory at
`/work/pi_<your_pi_name>/.apptainer/cache` and reload the module. 

==========
== CUDA ==
==========

CUDA Version 11.8.0

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

WARNING: The NVIDIA Driver was not detected.  GPU functionality will not be available.
   Use the NVIDIA Container Toolkit to start this container with GPU support; see
   https://docs.nvidia.com/datacenter/cloud-native/ .

here() starts at /work/pi_alc_umass_edu/spconway/scratch/bw
── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
✔ dplyr     1.1.4     ✔ readr     2.1.5
✔ forcats   1.0.0     ✔ stringr   1.5.1
✔ ggplot2   3.5.1     ✔ tibble    3.2.1
✔ lubridate 1.9.3     ✔ tidyr     1.3.1
✔ purrr     1.0.2     
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors
Loading required package: StanHeaders

rstan version 2.32.6 (Stan version 2.32.2)

For execution on a local, multicore CPU with excess RAM we recommend calling
options(mc.cores = parallel::detectCores()).
To avoid recompilation of unchanged Stan programs, we recommend calling
rstan_options(auto_write = TRUE)
For within-chain threading using `reduce_sum()` or `map_rect()` Stan functions,
change `threads_per_chain` option:
rstan_options(threads_per_chain = 1)


Attaching package: ‘rstan’

The following object is masked from ‘package:tidyr’:

    extract

This is bayesplot version 1.11.1
- Online documentation and vignettes at mc-stan.org/bayesplot
- bayesplot theme set to bayesplot::theme_default()
   * Does _not_ affect other ggplot2 plots
   * See ?bayesplot_theme_set for details on theme setting
Rows: 107727 Columns: 26
── Column specification ────────────────────────────────────────────────────────
Delimiter: ","
chr  (3): bw_cond, effect, set
dbl (23): sub_n, block_n, trial_n, distance, diag, h1, w1, h2, w2, h3, w3, a...

ℹ Use `spec()` to retrieve the full column specification for this data.
ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
`summarise()` has grouped output by 'sub_n', 'set', 'distance'. You can
override using the `.groups` argument.
Joining with `by = join_by(sub_n)`
[1] 1
[1] 2
[1] 3
[1] 4
[1] 5
[1] 6
[1] 7
[1] 8
[1] 9
[1] 10
[1] 11
[1] 12
[1] 13
[1] 14
[1] 15
[1] 16
[1] 17
[1] 18
[1] 19
[1] 20
[1] 21
[1] 22
[1] 23
[1] 24
[1] 25
[1] 26
[1] 27
[1] 28
[1] 29
[1] 30
[1] 31
[1] 32
[1] 33
[1] 34
[1] 35
[1] 36
[1] 37
[1] 38
[1] 39
[1] 40
[1] 41
[1] 42
[1] 43
[1] 44
[1] 45
[1] 46
[1] 47
[1] 48
[1] 49
[1] 50
[1] 51
[1] 52
[1] 53
[1] 54
[1] 55
[1] 56
[1] 57
[1] 58
[1] 59
[1] 60
[1] 61
[1] 62
[1] 63
[1] 64
[1] 65
[1] 66
[1] 67
[1] 68
[1] 69
[1] 70
[1] 71
[1] 72
[1] 73
[1] 74
[1] 75
[1] 76
[1] 77
[1] 78
[1] 79
[1] 80
[1] 81
[1] 82
[1] 83
[1] 84
[1] 85
[1] 86
[1] 87
[1] 88
[1] 89
[1] 90
[1] 91
[1] 92
[1] 93
[1] 94
[1] 95
[1] 96
[1] 97
[1] 98
[1] 99
[1] 100
[1] 101
[1] 102
[1] 103
[1] 104
[1] 105
[1] 106
[1] 107
[1] 108
[1] 109
[1] 110
[1] 111
[1] 112
[1] 113
[1] 114
[1] 115
[1] 116
[1] 117
[1] 118
[1] 119
[1] 120
[1] 121
[1] 122
[1] 123
[1] 124
[1] 125
[1] 126
[1] 127
[1] 128
[1] 129
[1] 130
[1] 131
[1] 132
[1] 133
[1] 134
[1] 135
[1] 136
[1] 137
[1] 138
[1] 139
[1] 140
[1] 141
[1] 142
[1] 143
[1] 144
[1] 145
[1] 146
[1] 147
[1] 148
[1] 149
[1] 150
[1] 151
[1] 152
[1] 153
[1] 154
[1] 155
[1] 156
[1] 157
[1] 158
[1] 159
[1] 160
[1] 161
[1] 162
[1] 163
[1] 164
[1] 165
[1] 166
[1] 167
[1] 168
[1] 169
[1] 170
[1] 171
[1] 172
[1] 173
[1] 174
[1] 175
[1] 176
[1] 177
[1] 178
[1] 179
[1] 180
[1] 181
[1] 182
[1] 183
[1] 184
[1] 185
[1] 186
[1] 187
[1] 188
[1] 189
[1] 190
[1] 191
[1] 192
[1] 193
[1] 194
[1] 195
[1] 196
[1] 197
[1] 198
[1] 199
[1] 200
[1] 201
[1] 202
[1] 203
[1] 204
[1] 205
[1] 206
[1] 207
[1] 208
[1] 209
[1] 210
[1] 211
[1] 212
[1] 213
[1] 214
[1] 215
[1] 216
[1] 217
[1] 218
[1] 219
[1] 220
[1] 221
[1] 222
[1] 223
[1] 224
[1] 225
[1] 226
[1] 227
[1] 228
[1] 229
[1] 230
[1] 231
[1] 232
[1] 233
[1] 234
[1] 235
[1] 236
[1] 237
[1] 238
[1] 239
[1] 240
[1] 241
[1] 242
[1] 243
[1] 244
[1] 245
[1] 246
[1] 247
[1] 248
[1] 249
[1] 250
[1] 251
[1] 252
[1] 253
[1] 254
[1] 255
[1] 256
[1] 257
[1] 258
[1] 259
[1] 260
[1] 261
[1] 262
[1] 263
[1] 264
[1] 265
[1] 266
[1] 267
[1] 268
[1] 269
[1] 270
[1] 271
[1] 272
[1] 273
[1] 274
[1] 275
[1] 276
[1] 277
[1] 278
[1] 279
[1] 280
[1] 281
[1] 282
[1] 283
[1] 284
[1] 285
[1] 286
[1] 287
[1] 288
[1] 289
[1] 290
[1] 291
[1] 292
[1] 293
[1] 294
[1] 295
[1] 296
[1] 297
[1] 298
[1] 299
[1] 300
[1] 301
[1] 302
[1] 303
[1] 304
[1] 305
[1] 306
[1] 307
[1] 308
[1] 309
[1] 310
[1] 311
[1] 312
[1] 313
[1] 314
[1] 315
[1] 316
[1] 317
[1] 318
[1] 319
[1] 320
[1] 321
[1] 322
[1] 323
[1] 324
[1] 325
[1] 326
[1] 327
[1] 328
[1] 329
[1] 330
[1] 331
[1] 332
[1] 333
[1] 334
[1] 335
[1] 336
[1] 337
[1] 338
[1] 339
[1] 340
[1] 341
[1] 342
[1] 343
[1] 344
[1] 345
[1] 346
[1] 347
[1] 348
[1] 349
[1] 350
[1] 351
[1] 352
[1] 353
[1] 354
[1] 355
[1] 356
[1] 357
[1] 358
[1] 359
[1] 360
[1] 361
[1] 362
[1] 363
[1] 364
[1] 365
[1] 366
[1] 367
[1] 368
[1] 369

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).
Chain 1: 
Chain 1: Gradient evaluation took 0.008736 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 87.36 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:    1 / 3000 [  0%]  (Warmup)
Chain 3: 
Chain 3: Gradient evaluation took 0.008802 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 88.02 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 4: 
Chain 4: Gradient evaluation took 0.008288 seconds
Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 82.88 seconds.
Chain 4: Adjust your expectations accordingly!
Chain 4: 
Chain 4: 
Chain 3: Iteration:    1 / 3000 [  0%]  (Warmup)
Chain 4: Iteration:    1 / 3000 [  0%]  (Warmup)
Chain 2: 
Chain 2: Gradient evaluation took 0.00856 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 85.6 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:    1 / 3000 [  0%]  (Warmup)
Chain 2: Iteration:  300 / 3000 [ 10%]  (Warmup)
Chain 4: Iteration:  300 / 3000 [ 10%]  (Warmup)
Chain 1: Iteration:  300 / 3000 [ 10%]  (Warmup)
Chain 3: Iteration:  300 / 3000 [ 10%]  (Warmup)
Chain 2: Iteration:  600 / 3000 [ 20%]  (Warmup)
Chain 4: Iteration:  600 / 3000 [ 20%]  (Warmup)
Chain 1: Iteration:  600 / 3000 [ 20%]  (Warmup)
Chain 3: Iteration:  600 / 3000 [ 20%]  (Warmup)
Chain 2: Iteration:  900 / 3000 [ 30%]  (Warmup)
Chain 4: Iteration:  900 / 3000 [ 30%]  (Warmup)
Chain 1: Iteration:  900 / 3000 [ 30%]  (Warmup)
Chain 3: Iteration:  900 / 3000 [ 30%]  (Warmup)
Chain 2: Iteration: 1200 / 3000 [ 40%]  (Warmup)
Chain 4: Iteration: 1200 / 3000 [ 40%]  (Warmup)
Chain 1: Iteration: 1200 / 3000 [ 40%]  (Warmup)
Chain 3: Iteration: 1200 / 3000 [ 40%]  (Warmup)
Chain 2: Iteration: 1500 / 3000 [ 50%]  (Warmup)
Chain 2: Iteration: 1501 / 3000 [ 50%]  (Sampling)
Chain 4: Iteration: 1500 / 3000 [ 50%]  (Warmup)
Chain 4: Iteration: 1501 / 3000 [ 50%]  (Sampling)
Chain 1: Iteration: 1500 / 3000 [ 50%]  (Warmup)
Chain 1: Iteration: 1501 / 3000 [ 50%]  (Sampling)
Chain 3: Iteration: 1500 / 3000 [ 50%]  (Warmup)
Chain 3: Iteration: 1501 / 3000 [ 50%]  (Sampling)
Chain 2: Iteration: 1800 / 3000 [ 60%]  (Sampling)
Chain 4: Iteration: 1800 / 3000 [ 60%]  (Sampling)
Chain 1: Iteration: 1800 / 3000 [ 60%]  (Sampling)
Chain 3: Iteration: 1800 / 3000 [ 60%]  (Sampling)
Chain 2: Iteration: 2100 / 3000 [ 70%]  (Sampling)
Chain 4: Iteration: 2100 / 3000 [ 70%]  (Sampling)
Chain 1: Iteration: 2100 / 3000 [ 70%]  (Sampling)
Chain 3: Iteration: 2100 / 3000 [ 70%]  (Sampling)
Chain 2: Iteration: 2400 / 3000 [ 80%]  (Sampling)
Chain 4: Iteration: 2400 / 3000 [ 80%]  (Sampling)
Chain 1: Iteration: 2400 / 3000 [ 80%]  (Sampling)
Chain 3: Iteration: 2400 / 3000 [ 80%]  (Sampling)
Chain 2: Iteration: 2700 / 3000 [ 90%]  (Sampling)
Chain 4: Iteration: 2700 / 3000 [ 90%]  (Sampling)
Chain 1: Iteration: 2700 / 3000 [ 90%]  (Sampling)
Chain 3: Iteration: 2700 / 3000 [ 90%]  (Sampling)
Chain 2: Iteration: 3000 / 3000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 636.047 seconds (Warm-up)
Chain 2:                310.179 seconds (Sampling)
Chain 2:                946.226 seconds (Total)
Chain 2: 
Chain 4: Iteration: 3000 / 3000 [100%]  (Sampling)
Chain 4: 
Chain 4:  Elapsed Time: 648.883 seconds (Warm-up)
Chain 4:                313.323 seconds (Sampling)
Chain 4:                962.206 seconds (Total)
Chain 4: 
Chain 1: Iteration: 3000 / 3000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 669.758 seconds (Warm-up)
Chain 1:                329.636 seconds (Sampling)
Chain 1:                999.394 seconds (Total)
Chain 1: 
Chain 3: Iteration: 3000 / 3000 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 671.083 seconds (Warm-up)
Chain 3:                331.354 seconds (Sampling)
Chain 3:                1002.44 seconds (Total)
Chain 3: 
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
 0.9994  0.9997  0.9999  0.9999  1.0001  1.0073 
[1] 1
[1] 2
[1] 3
[1] 4
[1] 1
[1] 1
[1] 2
[1] 3
[1] 4
[1] 2
[1] 1
[1] 2
[1] 3
[1] 4
[1] 3
[1] 1
[1] 2
[1] 3
[1] 4
[1] 4
[1] 1
[1] 2
[1] 3
[1] 4
[1] 5
[1] 1
[1] 2
[1] 3
[1] 4
[1] 6
[1] 1
[1] 2
[1] 3
[1] 4
[1] 7
[1] 1
[1] 2
[1] 3
[1] 4
[1] 8
[1] 1
[1] 2
[1] 3
[1] 4
[1] 9
[1] 1
[1] 2
[1] 3
[1] 4
[1] 10
[1] 1
[1] 2
[1] 3
[1] 4
[1] 11
[1] 1
[1] 2
[1] 3
[1] 4
[1] 12
[1] 1
[1] 2
[1] 3
[1] 4
[1] 13
[1] 1
[1] 2
[1] 3
[1] 4
[1] 14
[1] 1
[1] 2
[1] 3
[1] 4
[1] 15
[1] 1
[1] 2
[1] 3
[1] 4
[1] 16
[1] 1
[1] 2
[1] 3
[1] 4
[1] 17
[1] 1
[1] 2
[1] 3
[1] 4
[1] 18
[1] 1
[1] 2
[1] 3
[1] 4
[1] 19
[1] 1
[1] 2
[1] 3
[1] 4
[1] 20
[1] 1
[1] 2
[1] 3
[1] 4
[1] 21
[1] 1
[1] 2
[1] 3
[1] 4
[1] 22
[1] 1
[1] 2
[1] 3
[1] 4
[1] 23
[1] 1
[1] 2
[1] 3
[1] 4
[1] 24
[1] 1
[1] 2
[1] 3
[1] 4
[1] 25
[1] 1
[1] 2
[1] 3
[1] 4
[1] 26
[1] 1
[1] 2
[1] 3
[1] 4
[1] 27
[1] 1
[1] 2
[1] 3
[1] 4
[1] 28
[1] 1
[1] 2
[1] 3
[1] 4
[1] 29
[1] 1
[1] 2
[1] 3
[1] 4
[1] 30
[1] 1
[1] 2
[1] 3
[1] 4
[1] 31
[1] 1
[1] 2
[1] 3
[1] 4
[1] 32
[1] 1
[1] 2
[1] 3
[1] 4
[1] 33
[1] 1
[1] 2
[1] 3
[1] 4
[1] 34
[1] 1
[1] 2
[1] 3
[1] 4
[1] 35
[1] 1
[1] 2
[1] 3
[1] 4
[1] 36
[1] 1
[1] 2
[1] 3
[1] 4
[1] 37
[1] 1
[1] 2
[1] 3
[1] 4
[1] 38
[1] 1
[1] 2
[1] 3
[1] 4
[1] 39
[1] 1
[1] 2
[1] 3
[1] 4
[1] 40
[1] 1
[1] 2
[1] 3
[1] 4
[1] 41
[1] 1
[1] 2
[1] 3
[1] 4
[1] 42
[1] 1
[1] 2
[1] 3
[1] 4
[1] 43
[1] 1
[1] 2
[1] 3
[1] 4
[1] 44
[1] 1
[1] 2
[1] 3
[1] 4
[1] 45
[1] 1
[1] 2
[1] 3
[1] 4
[1] 46
[1] 1
[1] 2
[1] 3
[1] 4
[1] 47
[1] 1
[1] 2
[1] 3
[1] 4
[1] 48
[1] 1
[1] 2
[1] 3
[1] 4
[1] 49
[1] 1
[1] 2
[1] 3
[1] 4
[1] 50
[1] 1
[1] 2
[1] 3
[1] 4
[1] 51
[1] 1
[1] 2
[1] 3
[1] 4
[1] 52
[1] 1
[1] 2
[1] 3
[1] 4
[1] 53
[1] 1
[1] 2
[1] 3
[1] 4
[1] 54
[1] 1
[1] 2
[1] 3
[1] 4
[1] 55
[1] 1
[1] 2
[1] 3
[1] 4
[1] 56
[1] 1
[1] 2
[1] 3
[1] 4
[1] 57
[1] 1
[1] 2
[1] 3
[1] 4
[1] 58
[1] 1
[1] 2
[1] 3
[1] 4
[1] 59
[1] 1
[1] 2
[1] 3
[1] 4
[1] 60
[1] 1
[1] 2
[1] 3
[1] 4
[1] 61
[1] 1
[1] 2
[1] 3
[1] 4
[1] 62
[1] 1
[1] 2
[1] 3
[1] 4
[1] 63
[1] 1
[1] 2
[1] 3
[1] 4
[1] 64
[1] 1
[1] 2
[1] 3
[1] 4
[1] 65
[1] 1
[1] 2
[1] 3
[1] 4
[1] 66
[1] 1
[1] 2
[1] 3
[1] 4
[1] 67
[1] 1
[1] 2
[1] 3
[1] 4
[1] 68
[1] 1
[1] 2
[1] 3
[1] 4
[1] 69
[1] 1
[1] 2
[1] 3
[1] 4
[1] 70
[1] 1
[1] 2
[1] 3
[1] 4
[1] 71
[1] 1
[1] 2
[1] 3
[1] 4
[1] 72
[1] 1
[1] 2
[1] 3
[1] 4
[1] 73
[1] 1
[1] 2
[1] 3
[1] 4
[1] 74
[1] 1
[1] 2
[1] 3
[1] 4
[1] 75
[1] 1
[1] 2
[1] 3
[1] 4
[1] 76
[1] 1
[1] 2
[1] 3
[1] 4
[1] 77
[1] 1
[1] 2
[1] 3
[1] 4
[1] 78
[1] 1
[1] 2
[1] 3
[1] 4
[1] 79
[1] 1
[1] 2
[1] 3
[1] 4
[1] 80
[1] 1
[1] 2
[1] 3
[1] 4
[1] 81
[1] 1
[1] 2
[1] 3
[1] 4
[1] 82
[1] 1
[1] 2
[1] 3
[1] 4
[1] 83
[1] 1
[1] 2
[1] 3
[1] 4
[1] 84
[1] 1
[1] 2
[1] 3
[1] 4
[1] 85
[1] 1
[1] 2
[1] 3
[1] 4
[1] 86
[1] 1
[1] 2
[1] 3
[1] 4
[1] 87
[1] 1
[1] 2
[1] 3
[1] 4
[1] 88
[1] 1
[1] 2
[1] 3
[1] 4
[1] 89
[1] 1
[1] 2
[1] 3
[1] 4
[1] 90
[1] 1
[1] 2
[1] 3
[1] 4
[1] 91
[1] 1
[1] 2
[1] 3
[1] 4
[1] 92
[1] 1
[1] 2
[1] 3
[1] 4
[1] 93
[1] 1
[1] 2
[1] 3
[1] 4
[1] 94
[1] 1
[1] 2
[1] 3
[1] 4
[1] 95
[1] 1
[1] 2
[1] 3
[1] 4
[1] 96
[1] 1
[1] 2
[1] 3
[1] 4
[1] 97
[1] 1
[1] 2
[1] 3
[1] 4
[1] 98
[1] 1
[1] 2
[1] 3
[1] 4
[1] 99
[1] 1
[1] 2
[1] 3
[1] 4
[1] 100
[1] 1
[1] 2
[1] 3
[1] 4
[1] 101
[1] 1
[1] 2
[1] 3
[1] 4
[1] 102
[1] 1
[1] 2
[1] 3
[1] 4
[1] 103
[1] 1
[1] 2
[1] 3
[1] 4
[1] 104
[1] 1
[1] 2
[1] 3
[1] 4
[1] 105
[1] 1
[1] 2
[1] 3
[1] 4
[1] 106
[1] 1
[1] 2
[1] 3
[1] 4
[1] 107
[1] 1
[1] 2
[1] 3
[1] 4
[1] 108
[1] 1
[1] 2
[1] 3
[1] 4
[1] 109
[1] 1
[1] 2
[1] 3
[1] 4
[1] 110
[1] 1
[1] 2
[1] 3
[1] 4
[1] 111
[1] 1
[1] 2
[1] 3
[1] 4
[1] 112
[1] 1
[1] 2
[1] 3
[1] 4
[1] 113
[1] 1
[1] 2
[1] 3
[1] 4
[1] 114
[1] 1
[1] 2
[1] 3
[1] 4
[1] 115
[1] 1
[1] 2
[1] 3
[1] 4
[1] 116
[1] 1
[1] 2
[1] 3
[1] 4
[1] 117
[1] 1
[1] 2
[1] 3
[1] 4
[1] 118
[1] 1
[1] 2
[1] 3
[1] 4
[1] 119
[1] 1
[1] 2
[1] 3
[1] 4
[1] 120
[1] 1
[1] 2
[1] 3
[1] 4
[1] 121
[1] 1
[1] 2
[1] 3
[1] 4
[1] 122
[1] 1
[1] 2
[1] 3
[1] 4
[1] 123
[1] 1
[1] 2
[1] 3
[1] 4
[1] 124
[1] 1
[1] 2
[1] 3
[1] 4
[1] 125
[1] 1
[1] 2
[1] 3
[1] 4
[1] 126
[1] 1
[1] 2
[1] 3
[1] 4
[1] 127
[1] 1
[1] 2
[1] 3
[1] 4
[1] 128
[1] 1
[1] 2
[1] 3
[1] 4
[1] 129
[1] 1
[1] 2
[1] 3
[1] 4
[1] 130
[1] 1
[1] 2
[1] 3
[1] 4
[1] 131
[1] 1
[1] 2
[1] 3
[1] 4
[1] 132
[1] 1
[1] 2
[1] 3
[1] 4
[1] 133
[1] 1
[1] 2
[1] 3
[1] 4
[1] 134
[1] 1
[1] 2
[1] 3
[1] 4
[1] 135
[1] 1
[1] 2
[1] 3
[1] 4
[1] 136
[1] 1
[1] 2
[1] 3
[1] 4
[1] 137
[1] 1
[1] 2
[1] 3
[1] 4
[1] 138
[1] 1
[1] 2
[1] 3
[1] 4
[1] 139
[1] 1
[1] 2
[1] 3
[1] 4
[1] 140
[1] 1
[1] 2
[1] 3
[1] 4
[1] 141
[1] 1
[1] 2
[1] 3
[1] 4
[1] 142
[1] 1
[1] 2
[1] 3
[1] 4
[1] 143
[1] 1
[1] 2
[1] 3
[1] 4
[1] 144
[1] 1
[1] 2
[1] 3
[1] 4
[1] 145
[1] 1
[1] 2
[1] 3
[1] 4
[1] 146
[1] 1
[1] 2
[1] 3
[1] 4
[1] 147
[1] 1
[1] 2
[1] 3
[1] 4
[1] 148
[1] 1
[1] 2
[1] 3
[1] 4
[1] 149
[1] 1
[1] 2
[1] 3
[1] 4
[1] 150
[1] 1
[1] 2
[1] 3
[1] 4
[1] 151
[1] 1
[1] 2
[1] 3
[1] 4
[1] 152
[1] 1
[1] 2
[1] 3
[1] 4
[1] 153
[1] 1
[1] 2
[1] 3
[1] 4
[1] 154
[1] 1
[1] 2
[1] 3
[1] 4
[1] 155
[1] 1
[1] 2
[1] 3
[1] 4
[1] 156
[1] 1
[1] 2
[1] 3
[1] 4
[1] 157
[1] 1
[1] 2
[1] 3
[1] 4
[1] 158
[1] 1
[1] 2
[1] 3
[1] 4
[1] 159
[1] 1
[1] 2
[1] 3
[1] 4
[1] 160
[1] 1
[1] 2
[1] 3
[1] 4
[1] 161
[1] 1
[1] 2
[1] 3
[1] 4
[1] 162
[1] 1
[1] 2
[1] 3
[1] 4
[1] 163
[1] 1
[1] 2
[1] 3
[1] 4
[1] 164
[1] 1
[1] 2
[1] 3
[1] 4
[1] 165
[1] 1
[1] 2
[1] 3
[1] 4
[1] 166
[1] 1
[1] 2
[1] 3
[1] 4
[1] 167
[1] 1
[1] 2
[1] 3
[1] 4
[1] 168
[1] 1
[1] 2
[1] 3
[1] 4
[1] 169
[1] 1
[1] 2
[1] 3
[1] 4
[1] 170
[1] 1
[1] 2
[1] 3
[1] 4
[1] 171
[1] 1
[1] 2
[1] 3
[1] 4
[1] 172
[1] 1
[1] 2
[1] 3
[1] 4
[1] 173
[1] 1
[1] 2
[1] 3
[1] 4
[1] 174
[1] 1
[1] 2
[1] 3
[1] 4
[1] 175
[1] 1
[1] 2
[1] 3
[1] 4
[1] 176
[1] 1
[1] 2
[1] 3
[1] 4
[1] 177
[1] 1
[1] 2
[1] 3
[1] 4
[1] 178
[1] 1
[1] 2
[1] 3
[1] 4
[1] 179
[1] 1
[1] 2
[1] 3
[1] 4
[1] 180
[1] 1
[1] 2
[1] 3
[1] 4
[1] 181
[1] 1
[1] 2
[1] 3
[1] 4
[1] 182
[1] 1
[1] 2
[1] 3
[1] 4
[1] 183
[1] 1
[1] 2
[1] 3
[1] 4
[1] 184
[1] 1
[1] 2
[1] 3
[1] 4
[1] 185
[1] 1
[1] 2
[1] 3
[1] 4
[1] 186
[1] 1
[1] 2
[1] 3
[1] 4
[1] 187
[1] 1
[1] 2
[1] 3
[1] 4
[1] 188
[1] 1
[1] 2
[1] 3
[1] 4
[1] 189
[1] 1
[1] 2
[1] 3
[1] 4
[1] 190
[1] 1
[1] 2
[1] 3
[1] 4
[1] 191
[1] 1
[1] 2
[1] 3
[1] 4
[1] 192
[1] 1
[1] 2
[1] 3
[1] 4
[1] 193
[1] 1
[1] 2
[1] 3
[1] 4
[1] 194
[1] 1
[1] 2
[1] 3
[1] 4
[1] 195
[1] 1
[1] 2
[1] 3
[1] 4
[1] 196
[1] 1
[1] 2
[1] 3
[1] 4
[1] 197
[1] 1
[1] 2
[1] 3
[1] 4
[1] 198
[1] 1
[1] 2
[1] 3
[1] 4
[1] 199
[1] 1
[1] 2
[1] 3
[1] 4
[1] 200
[1] 1
[1] 2
[1] 3
[1] 4
[1] 201
[1] 1
[1] 2
[1] 3
[1] 4
[1] 202
[1] 1
[1] 2
[1] 3
[1] 4
[1] 203
[1] 1
[1] 2
[1] 3
[1] 4
[1] 204
[1] 1
[1] 2
[1] 3
[1] 4
[1] 205
[1] 1
[1] 2
[1] 3
[1] 4
[1] 206
[1] 1
[1] 2
[1] 3
[1] 4
[1] 207
[1] 1
[1] 2
[1] 3
[1] 4
[1] 208
[1] 1
[1] 2
[1] 3
[1] 4
[1] 209
[1] 1
[1] 2
[1] 3
[1] 4
[1] 210
[1] 1
[1] 2
[1] 3
[1] 4
[1] 211
[1] 1
[1] 2
[1] 3
[1] 4
[1] 212
[1] 1
[1] 2
[1] 3
[1] 4
[1] 213
[1] 1
[1] 2
[1] 3
[1] 4
[1] 214
[1] 1
[1] 2
[1] 3
[1] 4
[1] 215
[1] 1
[1] 2
[1] 3
[1] 4
[1] 216
[1] 1
[1] 2
[1] 3
[1] 4
[1] 217
[1] 1
[1] 2
[1] 3
[1] 4
[1] 218
[1] 1
[1] 2
[1] 3
[1] 4
[1] 219
[1] 1
[1] 2
[1] 3
[1] 4
[1] 220
[1] 1
[1] 2
[1] 3
[1] 4
[1] 221
[1] 1
[1] 2
[1] 3
[1] 4
[1] 222
[1] 1
[1] 2
[1] 3
[1] 4
[1] 223
[1] 1
[1] 2
[1] 3
[1] 4
[1] 224
[1] 1
[1] 2
[1] 3
[1] 4
[1] 225
[1] 1
[1] 2
[1] 3
[1] 4
[1] 226
[1] 1
[1] 2
[1] 3
[1] 4
[1] 227
[1] 1
[1] 2
[1] 3
[1] 4
[1] 228
[1] 1
[1] 2
[1] 3
[1] 4
[1] 229
[1] 1
[1] 2
[1] 3
[1] 4
[1] 230
[1] 1
[1] 2
[1] 3
[1] 4
[1] 231
[1] 1
[1] 2
[1] 3
[1] 4
[1] 232
[1] 1
[1] 2
[1] 3
[1] 4
[1] 233
[1] 1
[1] 2
[1] 3
[1] 4
[1] 234
[1] 1
[1] 2
[1] 3
[1] 4
[1] 235
[1] 1
[1] 2
[1] 3
[1] 4
[1] 236
[1] 1
[1] 2
[1] 3
[1] 4
[1] 237
[1] 1
[1] 2
[1] 3
[1] 4
[1] 238
[1] 1
[1] 2
[1] 3
[1] 4
[1] 239
[1] 1
[1] 2
[1] 3
[1] 4
[1] 240
[1] 1
[1] 2
[1] 3
[1] 4
[1] 241
[1] 1
[1] 2
[1] 3
[1] 4
[1] 242
[1] 1
[1] 2
[1] 3
[1] 4
[1] 243
[1] 1
[1] 2
[1] 3
[1] 4
[1] 244
[1] 1
[1] 2
[1] 3
[1] 4
[1] 245
[1] 1
[1] 2
[1] 3
[1] 4
[1] 246
[1] 1
[1] 2
[1] 3
[1] 4
[1] 247
[1] 1
[1] 2
[1] 3
[1] 4
[1] 248
[1] 1
[1] 2
[1] 3
[1] 4
[1] 249
[1] 1
[1] 2
[1] 3
[1] 4
[1] 250
[1] 1
[1] 2
[1] 3
[1] 4
[1] 251
[1] 1
[1] 2
[1] 3
[1] 4
[1] 252
[1] 1
[1] 2
[1] 3
[1] 4
[1] 253
[1] 1
[1] 2
[1] 3
[1] 4
[1] 254
[1] 1
[1] 2
[1] 3
[1] 4
[1] 255
[1] 1
[1] 2
[1] 3
[1] 4
[1] 256
[1] 1
[1] 2
[1] 3
[1] 4
[1] 257
[1] 1
[1] 2
[1] 3
[1] 4
[1] 258
[1] 1
[1] 2
[1] 3
[1] 4
[1] 259
[1] 1
[1] 2
[1] 3
[1] 4
[1] 260
[1] 1
[1] 2
[1] 3
[1] 4
[1] 261
[1] 1
[1] 2
[1] 3
[1] 4
[1] 262
[1] 1
[1] 2
[1] 3
[1] 4
[1] 263
[1] 1
[1] 2
[1] 3
[1] 4
[1] 264
[1] 1
[1] 2
[1] 3
[1] 4
[1] 265
[1] 1
[1] 2
[1] 3
[1] 4
[1] 266
[1] 1
[1] 2
[1] 3
[1] 4
[1] 267
[1] 1
[1] 2
[1] 3
[1] 4
[1] 268
[1] 1
[1] 2
[1] 3
[1] 4
[1] 269
[1] 1
[1] 2
[1] 3
[1] 4
[1] 270
[1] 1
[1] 2
[1] 3
[1] 4
[1] 271
[1] 1
[1] 2
[1] 3
[1] 4
[1] 272
[1] 1
[1] 2
[1] 3
[1] 4
[1] 273
[1] 1
[1] 2
[1] 3
[1] 4
[1] 274
[1] 1
[1] 2
[1] 3
[1] 4
[1] 275
[1] 1
[1] 2
[1] 3
[1] 4
[1] 276
[1] 1
[1] 2
[1] 3
[1] 4
[1] 277
[1] 1
[1] 2
[1] 3
[1] 4
[1] 278
[1] 1
[1] 2
[1] 3
[1] 4
[1] 279
[1] 1
[1] 2
[1] 3
[1] 4
[1] 280
[1] 1
[1] 2
[1] 3
[1] 4
[1] 281
[1] 1
[1] 2
[1] 3
[1] 4
[1] 282
[1] 1
[1] 2
[1] 3
[1] 4
[1] 283
[1] 1
[1] 2
[1] 3
[1] 4
[1] 284
[1] 1
[1] 2
[1] 3
[1] 4
[1] 285
[1] 1
[1] 2
[1] 3
[1] 4
[1] 286
[1] 1
[1] 2
[1] 3
[1] 4
[1] 287
[1] 1
[1] 2
[1] 3
[1] 4
[1] 288
[1] 1
[1] 2
[1] 3
[1] 4
[1] 289
[1] 1
[1] 2
[1] 3
[1] 4
[1] 290
[1] 1
[1] 2
[1] 3
[1] 4
[1] 291
[1] 1
[1] 2
[1] 3
[1] 4
[1] 292
[1] 1
[1] 2
[1] 3
[1] 4
[1] 293
[1] 1
[1] 2
[1] 3
[1] 4
[1] 294
[1] 1
[1] 2
[1] 3
[1] 4
[1] 295
[1] 1
[1] 2
[1] 3
[1] 4
[1] 296
[1] 1
[1] 2
[1] 3
[1] 4
[1] 297
[1] 1
[1] 2
[1] 3
[1] 4
[1] 298
[1] 1
[1] 2
[1] 3
[1] 4
[1] 299
[1] 1
[1] 2
[1] 3
[1] 4
[1] 300
[1] 1
[1] 2
[1] 3
[1] 4
[1] 301
[1] 1
[1] 2
[1] 3
[1] 4
[1] 302
[1] 1
[1] 2
[1] 3
[1] 4
[1] 303
[1] 1
[1] 2
[1] 3
[1] 4
[1] 304
[1] 1
[1] 2
[1] 3
[1] 4
[1] 305
[1] 1
[1] 2
[1] 3
[1] 4
[1] 306
[1] 1
[1] 2
[1] 3
[1] 4
[1] 307
[1] 1
[1] 2
[1] 3
[1] 4
[1] 308
[1] 1
[1] 2
[1] 3
[1] 4
[1] 309
[1] 1
[1] 2
[1] 3
[1] 4
[1] 310
[1] 1
[1] 2
[1] 3
[1] 4
[1] 311
[1] 1
[1] 2
[1] 3
[1] 4
[1] 312
[1] 1
[1] 2
[1] 3
[1] 4
[1] 313
[1] 1
[1] 2
[1] 3
[1] 4
[1] 314
[1] 1
[1] 2
[1] 3
[1] 4
[1] 315
[1] 1
[1] 2
[1] 3
[1] 4
[1] 316
[1] 1
[1] 2
[1] 3
[1] 4
[1] 317
[1] 1
[1] 2
[1] 3
[1] 4
[1] 318
[1] 1
[1] 2
[1] 3
[1] 4
[1] 319
[1] 1
[1] 2
[1] 3
[1] 4
[1] 320
[1] 1
[1] 2
[1] 3
[1] 4
[1] 321
[1] 1
[1] 2
[1] 3
[1] 4
[1] 322
[1] 1
[1] 2
[1] 3
[1] 4
[1] 323
[1] 1
[1] 2
[1] 3
[1] 4
[1] 324
[1] 1
[1] 2
[1] 3
[1] 4
[1] 325
[1] 1
[1] 2
[1] 3
[1] 4
[1] 326
[1] 1
[1] 2
[1] 3
[1] 4
[1] 327
[1] 1
[1] 2
[1] 3
[1] 4
[1] 328
[1] 1
[1] 2
[1] 3
[1] 4
[1] 329
[1] 1
[1] 2
[1] 3
[1] 4
[1] 330
[1] 1
[1] 2
[1] 3
[1] 4
[1] 331
[1] 1
[1] 2
[1] 3
[1] 4
[1] 332
[1] 1
[1] 2
[1] 3
[1] 4
[1] 333
[1] 1
[1] 2
[1] 3
[1] 4
[1] 334
[1] 1
[1] 2
[1] 3
[1] 4
[1] 335
[1] 1
[1] 2
[1] 3
[1] 4
[1] 336
[1] 1
[1] 2
[1] 3
[1] 4
[1] 337
[1] 1
[1] 2
[1] 3
[1] 4
[1] 338
[1] 1
[1] 2
[1] 3
[1] 4
[1] 339
[1] 1
[1] 2
[1] 3
[1] 4
[1] 340
[1] 1
[1] 2
[1] 3
[1] 4
[1] 341
[1] 1
[1] 2
[1] 3
[1] 4
[1] 342
[1] 1
[1] 2
[1] 3
[1] 4
[1] 343
[1] 1
[1] 2
[1] 3
[1] 4
[1] 344
[1] 1
[1] 2
[1] 3
[1] 4
[1] 345
[1] 1
[1] 2
[1] 3
[1] 4
[1] 346
[1] 1
[1] 2
[1] 3
[1] 4
[1] 347
[1] 1
[1] 2
[1] 3
[1] 4
[1] 348
[1] 1
[1] 2
[1] 3
[1] 4
[1] 349
[1] 1
[1] 2
[1] 3
[1] 4
[1] 350
[1] 1
[1] 2
[1] 3
[1] 4
[1] 351
[1] 1
[1] 2
[1] 3
[1] 4
[1] 352
[1] 1
[1] 2
[1] 3
[1] 4
[1] 353
[1] 1
[1] 2
[1] 3
[1] 4
[1] 354
[1] 1
[1] 2
[1] 3
[1] 4
[1] 355
[1] 1
[1] 2
[1] 3
[1] 4
[1] 356
[1] 1
[1] 2
[1] 3
[1] 4
[1] 357
[1] 1
[1] 2
[1] 3
[1] 4
[1] 358
[1] 1
[1] 2
[1] 3
[1] 4
[1] 359
[1] 1
[1] 2
[1] 3
[1] 4
[1] 360
[1] 1
[1] 2
[1] 3
[1] 4
[1] 361
[1] 1
[1] 2
[1] 3
[1] 4
[1] 362
[1] 1
[1] 2
[1] 3
[1] 4
[1] 363
[1] 1
[1] 2
[1] 3
[1] 4
[1] 364
[1] 1
[1] 2
[1] 3
[1] 4
[1] 365
[1] 1
[1] 2
[1] 3
[1] 4
[1] 366
[1] 1
[1] 2
[1] 3
[1] 4
[1] 367
[1] 1
[1] 2
[1] 3
[1] 4
[1] 368
[1] 1
[1] 2
[1] 3
[1] 4
[1] 369
[1] 1
[1] 2
[1] 3
[1] 4
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
# A tibble: 4,428 × 9
   sub_n distance option p_best_m p_best_m_l p_best_m_u p_worst_m p_worst_m_l
   <int>    <dbl> <chr>     <dbl>      <dbl>      <dbl>     <dbl>       <dbl>
 1     1        2 t         0.285     0.204       0.367     0.265      0.186 
 2     1        2 c         0.436     0.344       0.524     0.370      0.285 
 3     1        2 d         0.279     0.205       0.368     0.366      0.284 
 4     1        5 t         0.365     0.269       0.459     0.194      0.117 
 5     1        5 c         0.468     0.368       0.563     0.224      0.147 
 6     1        5 d         0.167     0.0959      0.240     0.582      0.486 
 7     1        9 t         0.379     0.281       0.482     0.181      0.100 
 8     1        9 c         0.476     0.376       0.584     0.208      0.124 
 9     1        9 d         0.145     0.0684      0.217     0.612      0.508 
10     1       14 t         0.436     0.338       0.541     0.101      0.0444
# ℹ 4,418 more rows
# ℹ 1 more variable: p_worst_m_u <dbl>
# A tibble: 68,673 × 31
   sub_n bw_cond block_n trial_n effect   set   distance  diag    h1    w1    h2
   <dbl> <chr>     <dbl>   <dbl> <chr>    <chr>    <dbl> <dbl> <dbl> <dbl> <dbl>
 1    22 wb            1       6 attract… h            9     2    90   165   165
 2    22 wb            1       8 attract… h            5     2   161    88    90
 3    22 wb            1       9 attract… w            5     2   165    90    90
 4    22 wb            1      12 attract… h            5     3   195   120   120
 5    22 wb            1      13 attract… h            2     2    90   165   163
 6    22 wb            1      14 attract… w            9     2    86   157    90
 7    22 wb            1      15 attract… h            2     1    60   135   135
 8    22 wb            1      16 attract… h            2     3   120   195   195
 9    22 wb            1      17 attract… h            5     1    60   135   132
10    22 wb            1      18 attract… w            5     3   195   120   120
# ℹ 68,663 more rows
# ℹ 20 more variables: w2 <dbl>, h3 <dbl>, w3 <dbl>, a1 <dbl>, a2 <dbl>,
#   a3 <dbl>, choice_best <dbl>, choice_worst <dbl>, rt_best <dbl>,
#   rt_worst <dbl>, a_max <dbl>, a_min <dbl>, choice_best_correct <dbl>,
#   choice_worst_correct <dbl>, both_correct <dbl>, min <int>, best <chr>,
#   worst <chr>, best_att <chr>, worst_att <chr>
`summarise()` has grouped output by 'distance', 'type', 'option'. You can
override using the `.groups` argument.
Joining with `by = join_by(sub_n)`
Warning message:
Removed 93 rows containing missing values or values outside the scale range
(`geom_point()`). 
Warning message:
Removed 93 rows containing missing values or values outside the scale range
(`geom_point()`). 
Warning message:
Removed 38 rows containing missing values or values outside the scale range
(`geom_point()`). 
Warning message:
Removed 38 rows containing missing values or values outside the scale range
(`geom_point()`). 
`summarise()` has grouped output by 'distance', 'option'. You can override
using the `.groups` argument.
[1] 0.1916984
`summarise()` has grouped output by 'distance'. You can override using the
`.groups` argument.
